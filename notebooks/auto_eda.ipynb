{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from scipy import stats\n",
    "import plotly.express as px\n",
    "import plotly.graph_objects as go\n",
    "from plotly.subplots import make_subplots\n",
    "import time\n",
    "import warnings\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.decomposition import PCA\n",
    "import networkx as nx\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "    def dimensionality_reduction(self, n_components=2, features_type='connectome', plot_targets=None):\n",
    "        \"\"\"\n",
    "        Perform dimensionality reduction on the dataset\n",
    "        \n",
    "        Parameters:\n",
    "        -----------\n",
    "        n_components : int\n",
    "            Number of components for PCA\n",
    "        features_type : str\n",
    "            Type of features to use ('connectome', 'survey', or 'all')\n",
    "        plot_targets : list\n",
    "            List of target columns to color the plot by\n",
    "        \"\"\"\n",
    "        if self.combined_df is None and features_type == 'all':\n",
    "            print(\"Error: No combined dataset available for 'all' features. Run combine_datasets() first.\")\n",
    "            return\n",
    "            \n",
    "        start_time = time.time()\n",
    "        \n",
    "        # Select features based on type\n",
    "        if features_type == 'connectome':\n",
    "            if self.connectome_df is None:\n",
    "                print(\"Error: No connectome data available\")\n",
    "                return\n",
    "            data = self.connectome_df\n",
    "        elif features_type == 'survey':\n",
    "            if self.survey_df is None:\n",
    "                print(\"Error: No survey data available\")\n",
    "                return\n",
    "            data = self.survey_df.select_dtypes(include=['int64', 'float64'])\n",
    "        elif features_type == 'all':\n",
    "            data = self.combined_df.select_dtypes(include=['int64', 'float64'])\n",
    "        else:\n",
    "            print(\"Error: Invalid features_type. Choose from 'connectome', 'survey', or 'all'\")\n",
    "            return\n",
    "        \n",
    "        # Remove any target columns from the features\n",
    "        if plot_targets:\n",
    "            data = data[[col for col in data.columns if col not in plot_targets]]\n",
    "        \n",
    "        # Handle missing values\n",
    "        data = data.fillna(data.mean())\n",
    "        \n",
    "        # Standardize the data\n",
    "        scaler = StandardScaler()\n",
    "        data_scaled = scaler.fit_transform(data)\n",
    "        \n",
    "        # PCA\n",
    "        pca = PCA(n_components=n_components)\n",
    "        pca_result = pca.fit_transform(data_scaled)\n",
    "        \n",
    "        print(f\"\\n=== PCA Analysis ({features_type} features) ===\")\n",
    "        print(f\"Explained variance ratio: {pca.explained_variance_ratio_}\")\n",
    "        print(f\"Total explained variance: {sum(pca.explained_variance_ratio_):.4f}\")\n",
    "        \n",
    "        # Create dataframe for plotting\n",
    "        pca_df = pd.DataFrame(data=pca_result, columns=[f'PC{i+1}' for i in range(n_components)])\n",
    "        \n",
    "        # Add target variables if provided\n",
    "        if plot_targets:\n",
    "            for target in plot_targets:\n",
    "                if features_type == 'all' and target in self.combined_df:\n",
    "                    pca_df[target] = self.combined_df[target].values\n",
    "                elif features_type == 'connectome' and self.target_df is not None and target in self.target_df:\n",
    "                    if len(pca_df) == len(self.target_df):\n",
    "                        pca_df[target] = self.target_df[target].values\n",
    "                elif features_type == 'survey' and self.target_df is not None and target in self.target_df:\n",
    "                    if len(pca_df) == len(self.target_df):\n",
    "                        pca_df[target] = self.target_df[target].values\n",
    "        \n",
    "        # # Visualize PCA results\n",
    "        # if n_components >= 2:\n",
    "        #     for target in plot_targets if plot_targets else [None]:\n",
    "        #         plt.figure(figsize=(10, 8))\n",
    "                \n",
    "        #         if target:\n",
    "        #             scatter = plt.scatter(pca_df['PC1'], pca_df['PC2'], \n",
    "        #                                  c=pca_df[target].astype('category').cat.codes if pca_df[target].dtype == 'object' else pca_df[target], \n",
    "        #                                  alpha=0.7, s=50, cmap='viridis')\n",
    "        #             if pca_df[target].dtype == 'object' or pca_df[target].nunique() < 10:\n",
    "        #                 plt.colorbar(scatter, label=target, ticks=range(pca_df[target].nunique()), \n",
    "        #                             format=plt.FuncFormatter(lambda i, *args: pca_df[target].unique()[int(i)] if i < len(pca_df[target].unique()) else ''))\n",
    "        #             else:\n",
    "        #                 plt.colorbar(scatter, label=target)\n",
    "        #         else:\n",
    "        #             plt.scatter(pca_df['PC1'], pca_df['PC2'], alpha=0.7)\n",
    "                \n",
    "        #         plt.title(f'PCA of {features_type.capitalize()} Features' + (f' by {target}' if target else ''))\n",
    "        #         plt.xlabel(f'PC1 ({pca.explained_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class ConnectomeProfiler:\n",
    "    \"\"\"\n",
    "    A class to expedite EDA for connectome data and related survey features\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, connectome_df=None, cat_df=None,quant_df=None, target_df=None):\n",
    "        \"\"\"\n",
    "        Initialize the profiler with connectome, survey, and target dataframes\n",
    "        \n",
    "        Parameters:\n",
    "        -----------\n",
    "        connectome_df : pandas DataFrame\n",
    "            DataFrame containing connectome features (brain region connections)\n",
    "        survey_df : pandas DataFrame\n",
    "            DataFrame containing survey responses and other metadata\n",
    "        target_df : pandas DataFrame\n",
    "            DataFrame containing target variables (ADHD diagnosis and sex)\n",
    "        \"\"\"\n",
    "        self.connectome_df = connectome_df\n",
    "        self.cat_df = cat_df\n",
    "        self.quant_df = quant_df\n",
    "        self.target_df = target_df\n",
    "        self.combined_df = None\n",
    "        self.performance_log = {}\n",
    "        \n",
    "    def load_data(self, connectome_path=None, cat_path=None, quant_path=None, target_path=None):\n",
    "        \"\"\"\n",
    "        Load data from files\n",
    "        \n",
    "        Parameters:\n",
    "        -----------\n",
    "        connectome_path : str\n",
    "            Path to connectome data CSV/parquet file\n",
    "        survey_path : str\n",
    "            Path to survey data CSV/parquet file\n",
    "        target_path : str\n",
    "            Path to target data CSV/parquet file\n",
    "        \"\"\"\n",
    "        start_time = time.time()\n",
    "        \n",
    "        if connectome_path:\n",
    "            if connectome_path.endswith('.csv'):\n",
    "                self.connectome_df = pd.read_csv(connectome_path)\n",
    "                \n",
    "        if quant_path:\n",
    "            if quant_path.endswith('.csv'):\n",
    "                self.quant_df = pd.read_csv(quant_path)\n",
    "        \n",
    "        if cat_path:\n",
    "            if cat_path.endswith('.csv'):\n",
    "                self.cat_df = pd.read_csv(cat_path)\n",
    "                \n",
    "        if target_path:\n",
    "            if target_path.endswith('.csv'):\n",
    "                self.target_df = pd.read_csv(target_path)\n",
    " \n",
    "                \n",
    "        self.performance_log['data_loading'] = time.time() - start_time\n",
    "        print(f\"Data loaded in {self.performance_log['data_loading']:.2f} seconds\")\n",
    "        \n",
    "        # Display basic info about the datasets\n",
    "        #self._show_data_summary()\n",
    "        # print(self.connectome_df.shape)\n",
    "        # print(self.cat_df.shape)\n",
    "        #return self.connectome_df\n",
    "        \n",
    "    # def _show_data_summary(self):\n",
    "    #     \"\"\"Display basic information about loaded datasets\"\"\"\n",
    "    #     print(\"\\n=== Data Summary ===\")\n",
    "        \n",
    "    #     if self.connectome_df is not None:\n",
    "    #         print(f\"\\nConnectome Data: {self.connectome_df.shape[0]} samples, {self.connectome_df.shape[1]} features\")\n",
    "    #         print(f\"Memory usage: {self.connectome_df.memory_usage().sum() / 1e6:.2f} MB\")\n",
    "        \n",
    "    #     if self.cat_df is not None:\n",
    "    #         print(f\"\\nSurvey Data: {self.cat_df.shape[0]} samples, {self.cat_df.shape[1]} features\")\n",
    "    #         print(f\"Memory usage: {self.cat_df.memory_usage().sum() / 1e6:.2f} MB\")\n",
    "    #     if self.quant_df is not None:\n",
    "    #         print(f\"\\nSurvey Data: {self.quant_df.shape[0]} samples, {self.quant_df.shape[1]} features\")\n",
    "    #         print(f\"Memory usage: {self.quant_df.memory_usage().sum() / 1e6:.2f} MB\")\n",
    "            \n",
    "    #     if self.target_df is not None:\n",
    "    #         print(f\"\\nTarget Data: {self.target_df.shape[0]} samples, {self.target_df.shape[1]} features\")\n",
    "    #         target_cols = self.target_df.columns.tolist()\n",
    "    #         print(f\"Target columns: {target_cols}\")\n",
    "            \n",
    "    #         # Show basic distribution of targets if they exist\n",
    "    #         for col in target_cols:\n",
    "    #             if self.target_df[col].nunique() < 10:  # Only for categorical/binary targets\n",
    "    #                 print(f\"\\nDistribution of {col}:\")\n",
    "    #                 display(self.target_df[col].value_counts(normalize=True).to_frame().T)\n",
    "    \n",
    "    def combine_datasets(self, id_column=None):\n",
    "        \"\"\"\n",
    "        Combine connectome, survey, and target data into a single dataframe\n",
    "        \n",
    "        Parameters:\n",
    "        -----------\n",
    "        id_column : str\n",
    "            Column name to use for merging datasets\n",
    "        \"\"\"\n",
    "        if id_column is None:\n",
    "            print(\"Error: Please provide an ID column for merging datasets\")\n",
    "            return\n",
    "            \n",
    "        dfs_to_merge = []\n",
    "        if self.connectome_df is not None and id_column in self.connectome_df.columns:\n",
    "            dfs_to_merge.append(self.connectome_df)\n",
    "        if self.cat_df is not None and id_column in self.cat_df.columns:\n",
    "            dfs_to_merge.append(self.cat_df)\n",
    "        if self.quant_df is not None and id_column in self.quant_df.columns:\n",
    "            dfs_to_merge.append(self.quant_df)\n",
    "        if self.target_df is not None and id_column in self.target_df.columns:\n",
    "            dfs_to_merge.append(self.target_df)\n",
    "            \n",
    "        if len(dfs_to_merge) < 2:\n",
    "            print(\"Error: Not enough valid dataframes with the ID column to merge\")\n",
    "            return\n",
    "            \n",
    "        # Start with the first dataframe\n",
    "        self.combined_df = dfs_to_merge[0].copy()\n",
    "        \n",
    "        # Merge with remaining dataframes\n",
    "        for df in dfs_to_merge[1:]:\n",
    "            self.combined_df = pd.merge(self.combined_df, df, on=id_column, how='inner')\n",
    "            \n",
    "        print(f\"Combined dataset created with {self.combined_df.shape[0]} samples and {self.combined_df.shape[1]} features\")\n",
    "        print(f\"Memory usage: {self.combined_df.memory_usage().sum() / 1e6:.2f} MB\")\n",
    "        \n",
    "        # Check for data loss during merge\n",
    "        for i, df in enumerate(dfs_to_merge):\n",
    "            loss = (1 - self.combined_df.shape[0] / df.shape[0]) * 100\n",
    "            if loss > 0:\n",
    "                print(f\"Warning: Lost {loss:.2f}% of samples from dataset {i+1} during merge\")\n",
    "                \n",
    "        return self.combined_df\n",
    "                \n",
    "    def profile_connectome(self, n_regions=None, corr_threshold=0.7, visualize=True):\n",
    "        \"\"\"\n",
    "        Profile connectome data to understand brain region connections\n",
    "        \n",
    "        Parameters:\n",
    "        -----------\n",
    "        n_regions : int\n",
    "            Number of brain regions to consider (for visualization purposes)\n",
    "        corr_threshold : float\n",
    "            Threshold for highlighting strong correlations\n",
    "        visualize : bool\n",
    "            Whether to generate visualizations\n",
    "        \"\"\"\n",
    "        if self.connectome_df is None:\n",
    "            print(\"Error: No connectome data loaded\")\n",
    "            return\n",
    "            \n",
    "        start_time = time.time()\n",
    "        \n",
    "        # Basic statistics\n",
    "        stats_df = pd.DataFrame({\n",
    "            'mean': self.connectome_df.mean(),\n",
    "            'std': self.connectome_df.std(),\n",
    "            'min': self.connectome_df.min(),\n",
    "            'max': self.connectome_df.max(),\n",
    "            'missing': self.connectome_df.isnull().sum(),\n",
    "            'missing_pct': self.connectome_df.isnull().mean() * 100\n",
    "        })\n",
    "        \n",
    "        print(\"\\n=== Connectome Data Profile ===\")\n",
    "        print(f\"Number of connections: {self.connectome_df.shape[1]}\")\n",
    "        print(f\"Missing values: {stats_df['missing'].sum()} ({stats_df['missing'].sum() / (self.connectome_df.shape[0] * self.connectome_df.shape[1]) * 100:.2f}%)\")\n",
    "        \n",
    "        # If column names follow a pattern like \"region1_region2\", extract unique regions\n",
    "        if n_regions is None and '_' in self.connectome_df.columns[0]:\n",
    "            regions = set()\n",
    "            for col in self.connectome_df.columns:\n",
    "                if '_' in col:\n",
    "                    r1, r2 = col.split('_')\n",
    "                    regions.add(r1)\n",
    "                    regions.add(r2)\n",
    "            n_regions = len(regions)\n",
    "            print(f\"Detected {n_regions} unique brain regions\")\n",
    "            \n",
    "        # Visualizations\n",
    "        if visualize:\n",
    "            # Distribution of connection strengths\n",
    "            plt.figure(figsize=(12, 6))\n",
    "            \n",
    "            # Overall distribution\n",
    "            plt.subplot(1, 2, 1)\n",
    "            self.connectome_df.mean().hist(bins=50)\n",
    "            plt.title('Distribution of Mean Connection Strengths')\n",
    "            plt.xlabel('Connection Strength')\n",
    "            plt.ylabel('Frequency')\n",
    "            \n",
    "            # Connection variability\n",
    "            plt.subplot(1, 2, 2)\n",
    "            self.connectome_df.std().hist(bins=50)\n",
    "            plt.title('Distribution of Connection Variability')\n",
    "            plt.xlabel('Standard Deviation')\n",
    "            plt.ylabel('Frequency')\n",
    "            \n",
    "            plt.tight_layout()\n",
    "            plt.show()\n",
    "            \n",
    "            # Correlation heatmap (subsample if too many features)\n",
    "            if n_regions and n_regions < 50:\n",
    "                sample_cols = np.random.choice(self.connectome_df.columns, min(n_regions**2, self.connectome_df.shape[1]), replace=False)\n",
    "                corr_matrix = self.connectome_df[sample_cols].corr()\n",
    "                \n",
    "                plt.figure(figsize=(12, 10))\n",
    "                mask = np.triu(np.ones_like(corr_matrix, dtype=bool))\n",
    "                sns.heatmap(corr_matrix, mask=mask, cmap='coolwarm', vmin=-1, vmax=1, \n",
    "                           square=True, linewidths=0.5, annot=False)\n",
    "                plt.title(f'Correlation Heatmap (Sample of {len(sample_cols)} Connections)')\n",
    "                plt.tight_layout()\n",
    "                plt.show()\n",
    "                \n",
    "                # Network graph of strong correlations\n",
    "                strong_corr = corr_matrix.abs().unstack()\n",
    "                strong_corr = strong_corr[strong_corr > corr_threshold]\n",
    "                strong_corr = strong_corr[strong_corr < 1.0]  # Remove self-correlations\n",
    "                \n",
    "                if len(strong_corr) > 0:\n",
    "                    G = nx.Graph()\n",
    "                    for (i, j), corr in strong_corr.items():\n",
    "                        G.add_edge(i, j, weight=corr)\n",
    "                        \n",
    "                    plt.figure(figsize=(12, 12))\n",
    "                    pos = nx.spring_layout(G, k=0.5, iterations=50)\n",
    "                    nx.draw_networkx_nodes(G, pos, node_size=100, alpha=0.8)\n",
    "                    nx.draw_networkx_edges(G, pos, width=1, alpha=0.5)\n",
    "                    nx.draw_networkx_labels(G, pos, font_size=8)\n",
    "                    plt.title(f'Network of Strong Correlations (r > {corr_threshold})')\n",
    "                    plt.axis('off')\n",
    "                    plt.tight_layout()\n",
    "                    plt.show()\n",
    "        \n",
    "        self.performance_log['connectome_profiling'] = time.time() - start_time\n",
    "        print(f\"Connectome profiling completed in {self.performance_log['connectome_profiling']:.2f} seconds\")\n",
    "        \n",
    "        return stats_df\n",
    "    \n",
    "    def profile_survey(self, max_categories=10, correlation_with=None):\n",
    "        \"\"\"\n",
    "        Profile survey data to understand distributions and relationships\n",
    "        \n",
    "        Parameters:\n",
    "        -----------\n",
    "        max_categories : int\n",
    "            Maximum number of categories to show for categorical variables\n",
    "        correlation_with : str\n",
    "            Target variable to calculate correlations with\n",
    "        \"\"\"\n",
    "        if self.survey_df is None:\n",
    "            print(\"Error: No survey data loaded\")\n",
    "            return\n",
    "            \n",
    "        start_time = time.time()\n",
    "        \n",
    "        # Identify data types\n",
    "        dtypes = self.survey_df.dtypes\n",
    "        numerical_cols = self.survey_df.select_dtypes(include=['int64', 'float64']).columns.tolist()\n",
    "        categorical_cols = self.survey_df.select_dtypes(include=['object', 'category', 'bool']).columns.tolist()\n",
    "        \n",
    "        print(\"\\n=== Survey Data Profile ===\")\n",
    "        print(f\"Numerical features: {len(numerical_cols)}\")\n",
    "        print(f\"Categorical features: {len(categorical_cols)}\")\n",
    "        \n",
    "        # Missing values\n",
    "        missing = self.survey_df.isnull().sum()\n",
    "        missing = missing[missing > 0]\n",
    "        missing_pct = missing / len(self.survey_df) * 100\n",
    "        \n",
    "        if len(missing) > 0:\n",
    "            missing_df = pd.DataFrame({'missing_values': missing, 'percentage': missing_pct})\n",
    "            missing_df = missing_df.sort_values('percentage', ascending=False)\n",
    "            \n",
    "            print(\"\\n=== Missing Values ===\")\n",
    "            display(missing_df.head(10))\n",
    "            \n",
    "            plt.figure(figsize=(12, 6))\n",
    "            missing_df.head(20)['percentage'].plot(kind='bar')\n",
    "            plt.title('Features with Most Missing Values')\n",
    "            plt.xlabel('Features')\n",
    "            plt.ylabel('Percentage Missing')\n",
    "            plt.xticks(rotation=90)\n",
    "            plt.tight_layout()\n",
    "            plt.show()\n",
    "        \n",
    "        # Numerical feature distributions\n",
    "        if len(numerical_cols) > 0:\n",
    "            print(\"\\n=== Numerical Feature Statistics ===\")\n",
    "            num_stats = self.survey_df[numerical_cols].describe().T\n",
    "            num_stats['missing'] = self.survey_df[numerical_cols].isnull().sum()\n",
    "            num_stats['missing_pct'] = self.survey_df[numerical_cols].isnull().mean() * 100\n",
    "            display(num_stats.head(10))\n",
    "            \n",
    "            # Plot distributions for a sample of numerical features\n",
    "            sample_num = min(6, len(numerical_cols))\n",
    "            if sample_num > 0:\n",
    "                sampled_num_cols = np.random.choice(numerical_cols, sample_num, replace=False)\n",
    "                \n",
    "                fig = plt.figure(figsize=(15, 3*sample_num))\n",
    "                for i, col in enumerate(sampled_num_cols):\n",
    "                    plt.subplot(sample_num, 2, 2*i+1)\n",
    "                    self.survey_df[col].hist(bins=30)\n",
    "                    plt.title(f'Distribution of {col}')\n",
    "                    \n",
    "                    plt.subplot(sample_num, 2, 2*i+2)\n",
    "                    sns.boxplot(x=self.survey_df[col])\n",
    "                    plt.title(f'Boxplot of {col}')\n",
    "                \n",
    "                plt.tight_layout()\n",
    "                plt.show()\n",
    "        \n",
    "        # Categorical feature distributions\n",
    "        if len(categorical_cols) > 0:\n",
    "            print(\"\\n=== Categorical Feature Statistics ===\")\n",
    "            cat_stats = pd.DataFrame({\n",
    "                'unique_values': [self.survey_df[col].nunique() for col in categorical_cols],\n",
    "                'missing': [self.survey_df[col].isnull().sum() for col in categorical_cols],\n",
    "                'missing_pct': [self.survey_df[col].isnull().mean() * 100 for col in categorical_cols]\n",
    "            }, index=categorical_cols)\n",
    "            display(cat_stats.sort_values('unique_values', ascending=False).head(10))\n",
    "            \n",
    "            # Plot distributions for a sample of categorical features\n",
    "            sample_cat = min(6, len(categorical_cols))\n",
    "            if sample_cat > 0:\n",
    "                # Select columns with a manageable number of categories\n",
    "                manageable_cats = [col for col in categorical_cols if self.survey_df[col].nunique() <= max_categories]\n",
    "                if len(manageable_cats) > 0:\n",
    "                    sampled_cat_cols = np.random.choice(manageable_cats, min(sample_cat, len(manageable_cats)), replace=False)\n",
    "                    \n",
    "                    fig = plt.figure(figsize=(15, 4*len(sampled_cat_cols)))\n",
    "                    for i, col in enumerate(sampled_cat_cols):\n",
    "                        plt.subplot(len(sampled_cat_cols), 1, i+1)\n",
    "                        value_counts = self.survey_df[col].value_counts().sort_values(ascending=False)\n",
    "                        value_counts.plot(kind='bar')\n",
    "                        plt.title(f'Distribution of {col}')\n",
    "                        plt.xticks(rotation=45)\n",
    "                    \n",
    "                    plt.tight_layout()\n",
    "                    plt.show()\n",
    "        \n",
    "        # Correlation with target if provided\n",
    "        if correlation_with is not None and correlation_with in self.survey_df.columns:\n",
    "            print(f\"\\n=== Correlation with {correlation_with} ===\")\n",
    "            \n",
    "            # For numerical features\n",
    "            if len(numerical_cols) > 0:\n",
    "                num_corrs = {}\n",
    "                for col in numerical_cols:\n",
    "                    if col != correlation_with:\n",
    "                        corr = self.survey_df[[col, correlation_with]].corr().iloc[0, 1]\n",
    "                        if not np.isnan(corr):\n",
    "                            num_corrs[col] = corr\n",
    "                \n",
    "                if num_corrs:\n",
    "                    num_corrs_df = pd.DataFrame({'correlation': num_corrs}).sort_values('correlation', ascending=False)\n",
    "                    \n",
    "                    # Display top positive and negative correlations\n",
    "                    print(\"Top positive correlations:\")\n",
    "                    display(num_corrs_df.head(10))\n",
    "                    \n",
    "                    print(\"Top negative correlations:\")\n",
    "                    display(num_corrs_df.tail(10))\n",
    "                    \n",
    "                    # Plot top correlations\n",
    "                    plt.figure(figsize=(12, 6))\n",
    "                    top_corrs = pd.concat([num_corrs_df.head(10), num_corrs_df.tail(10)])\n",
    "                    sns.barplot(x=top_corrs['correlation'], y=top_corrs.index)\n",
    "                    plt.title(f'Top Correlations with {correlation_with}')\n",
    "                    plt.axvline(x=0, color='r', linestyle='-')\n",
    "                    plt.tight_layout()\n",
    "                    plt.show()\n",
    "            \n",
    "            # For categorical features\n",
    "            if len(categorical_cols) > 0:\n",
    "                cat_assocs = {}\n",
    "                for col in categorical_cols:\n",
    "                    if col != correlation_with and self.survey_df[col].nunique() <= max_categories:\n",
    "                        # Calculate Cramer's V for association between categorical variables\n",
    "                        try:\n",
    "                            crosstab = pd.crosstab(self.survey_df[col], self.survey_df[correlation_with])\n",
    "                            chi2 = stats.chi2_contingency(crosstab)[0]\n",
    "                            n = crosstab.sum().sum()\n",
    "                            v = np.sqrt(chi2 / (n * (min(crosstab.shape) - 1)))\n",
    "                            cat_assocs[col] = v\n",
    "                        except:\n",
    "                            pass\n",
    "                \n",
    "                if cat_assocs:\n",
    "                    cat_assocs_df = pd.DataFrame({'association': cat_assocs}).sort_values('association', ascending=False)\n",
    "                    \n",
    "                    print(\"Top categorical associations:\")\n",
    "                    display(cat_assocs_df.head(10))\n",
    "                    \n",
    "                    # Plot top associations\n",
    "                    plt.figure(figsize=(12, 6))\n",
    "                    sns.barplot(x=cat_assocs_df['association'].head(10), y=cat_assocs_df.head(10).index)\n",
    "                    plt.title(f'Top Categorical Associations with {correlation_with}')\n",
    "                    plt.tight_layout()\n",
    "                    plt.show()\n",
    "        \n",
    "        self.performance_log['survey_profiling'] = time.time() - start_time\n",
    "        print(f\"Survey profiling completed in {self.performance_log['survey_profiling']:.2f} seconds\")\n",
    "    \n",
    "    def analyze_target_relationships(self, adhd_col=None, sex_col=None, n_features=20):\n",
    "        \"\"\"\n",
    "        Analyze relationships between predictors and target variables\n",
    "        \n",
    "        Parameters:\n",
    "        -----------\n",
    "        adhd_col : str\n",
    "            Column name for the ADHD diagnosis\n",
    "        sex_col : str\n",
    "            Column name for the sex classification\n",
    "        n_features : int\n",
    "            Number of top features to display\n",
    "        \"\"\"\n",
    "        if self.combined_df is None:\n",
    "            print(\"Error: No combined dataset available. Run combine_datasets() first.\")\n",
    "            return\n",
    "            \n",
    "        if adhd_col is None or sex_col is None:\n",
    "            print(\"Error: Please provide column names for ADHD and sex targets\")\n",
    "            return\n",
    "            \n",
    "        if adhd_col not in self.combined_df.columns or sex_col not in self.combined_df.columns:\n",
    "            print(\"Error: Target columns not found in the combined dataset\")\n",
    "            return\n",
    "            \n",
    "        start_time = time.time()\n",
    "        \n",
    "        print(\"\\n=== Target Relationship Analysis ===\")\n",
    "        \n",
    "        # Check relationship between the two targets\n",
    "        print(f\"\\nRelationship between {adhd_col} and {sex_col}:\")\n",
    "        crosstab = pd.crosstab(self.combined_df[adhd_col], self.combined_df[sex_col], normalize='all')\n",
    "        display(crosstab)\n",
    "        \n",
    "        # Chi-square test\n",
    "        chi2, p, _, _ = stats.chi2_contingency(pd.crosstab(self.combined_df[adhd_col], self.combined_df[sex_col]))\n",
    "        print(f\"Chi-square test: chi2={chi2:.2f}, p-value={p:.4f}\")\n",
    "        \n",
    "        # Split features by type\n",
    "        numerical_cols = self.combined_df.select_dtypes(include=['int64', 'float64']).columns.tolist()\n",
    "        numerical_cols = [col for col in numerical_cols if col not in [adhd_col, sex_col]]\n",
    "        \n",
    "        categorical_cols = self.combined_df.select_dtypes(include=['object', 'category', 'bool']).columns.tolist()\n",
    "        categorical_cols = [col for col in categorical_cols if col not in [adhd_col, sex_col]]\n",
    "        \n",
    "        # Analyze numerical features\n",
    "        if len(numerical_cols) > 0:\n",
    "            # Feature correlations with targets\n",
    "            adhd_corrs = {}\n",
    "            sex_corrs = {}\n",
    "            \n",
    "            for col in numerical_cols:\n",
    "                # For ADHD\n",
    "                adhd_corr = self.combined_df[[col, adhd_col]].corr().iloc[0, 1]\n",
    "                if not np.isnan(adhd_corr):\n",
    "                    adhd_corrs[col] = adhd_corr\n",
    "                    \n",
    "                # For sex\n",
    "                sex_corr = self.combined_df[[col, sex_col]].corr().iloc[0, 1]\n",
    "                if not np.isnan(sex_corr):\n",
    "                    sex_corrs[col] = sex_corr\n",
    "            \n",
    "            # Sort and display top correlations\n",
    "            adhd_corrs_df = pd.DataFrame({'correlation': adhd_corrs}).sort_values('correlation', key=abs, ascending=False)\n",
    "            sex_corrs_df = pd.DataFrame({'correlation': sex_corrs}).sort_values('correlation', key=abs, ascending=False)\n",
    "            \n",
    "            print(f\"\\nTop numerical features correlated with {adhd_col}:\")\n",
    "            display(adhd_corrs_df.head(n_features))\n",
    "            \n",
    "            print(f\"\\nTop numerical features correlated with {sex_col}:\")\n",
    "            display(sex_corrs_df.head(n_features))\n",
    "            \n",
    "            # Visualize correlations\n",
    "            plt.figure(figsize=(12, 10))\n",
    "            \n",
    "            # ADHD correlations\n",
    "            plt.subplot(2, 1, 1)\n",
    "            sns.barplot(x=adhd_corrs_df.head(n_features)['correlation'], y=adhd_corrs_df.head(n_features).index)\n",
    "            plt.title(f'Top Correlations with {adhd_col}')\n",
    "            plt.axvline(x=0, color='r', linestyle='-')\n",
    "            \n",
    "            # Sex correlations\n",
    "            plt.subplot(2, 1, 2)\n",
    "            sns.barplot(x=sex_corrs_df.head(n_features)['correlation'], y=sex_corrs_df.head(n_features).index)\n",
    "            plt.title(f'Top Correlations with {sex_col}')\n",
    "            plt.axvline(x=0, color='r', linestyle='-')\n",
    "            \n",
    "            plt.tight_layout()\n",
    "            plt.show()\n",
    "            \n",
    "            # Compare top features across targets\n",
    "            common_features = set(adhd_corrs_df.head(n_features).index) & set(sex_corrs_df.head(n_features).index)\n",
    "            \n",
    "            if common_features:\n",
    "                print(f\"\\nFeatures predictive of both targets:\")\n",
    "                comparison_df = pd.DataFrame({\n",
    "                    f'corr_with_{adhd_col}': [adhd_corrs[f] for f in common_features],\n",
    "                    f'corr_with_{sex_col}': [sex_corrs[f] for f in common_features]\n",
    "                }, index=common_features)\n",
    "                \n",
    "                display(comparison_df.sort_values(f'corr_with_{adhd_col}', key=abs, ascending=False))\n",
    "                \n",
    "                # Scatter plot of correlations\n",
    "                plt.figure(figsize=(10, 8))\n",
    "                plt.scatter([adhd_corrs[f] for f in common_features], \n",
    "                           [sex_corrs[f] for f in common_features], \n",
    "                           alpha=0.7)\n",
    "                \n",
    "                for f in common_features:\n",
    "                    plt.annotate(f, (adhd_corrs[f], sex_corrs[f]), fontsize=8)\n",
    "                    \n",
    "                plt.axhline(y=0, color='r', linestyle='-', alpha=0.3)\n",
    "                plt.axvline(x=0, color='r', linestyle='-', alpha=0.3)\n",
    "                plt.xlabel(f'Correlation with {adhd_col}')\n",
    "                plt.ylabel(f'Correlation with {sex_col}')\n",
    "                plt.title('Feature Correlation Comparison')\n",
    "                plt.grid(True, alpha=0.3)\n",
    "                plt.tight_layout()\n",
    "                plt.show()\n",
    "            \n",
    "            # Distribution of top features by target\n",
    "            for target, target_col in [(adhd_col, adhd_corrs_df), (sex_col, sex_corrs_df)]:\n",
    "                for feature in target_col.head(min(3, len(target_col))).index:\n",
    "                    plt.figure(figsize=(12, 5))\n",
    "                    \n",
    "                    plt.subplot(1, 2, 1)\n",
    "                    sns.boxplot(x=self.combined_df[target], y=self.combined_df[feature])\n",
    "                    plt.title(f'{feature} by {target}')\n",
    "                    \n",
    "                    plt.subplot(1, 2, 2)\n",
    "                    sns.histplot(data=self.combined_df, x=feature, hue=target, kde=True, element=\"step\")\n",
    "                    plt.title(f'Distribution of {feature} by {target}')\n",
    "                    \n",
    "                    plt.tight_layout()\n",
    "                    plt.show()\n",
    "        \n",
    "        self.performance_log['target_analysis'] = time.time() - start_time\n",
    "        print(f\"Target relationship analysis completed in {self.performance_log['target_analysis']:.2f} seconds\")\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data loaded in 4.75 seconds\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'NoneType' object has no attribute 'cat_df'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[35], line 4\u001b[0m\n\u001b[1;32m      1\u001b[0m ConnectomeProfiler()\u001b[39m.\u001b[39;49mload_data(\u001b[39m'\u001b[39;49m\u001b[39m/Users/paigegiese/SYG/wids-2025-mupd/data/TRAIN/TRAIN_FUNCTIONAL_CONNECTOME_MATRICES.csv\u001b[39;49m\u001b[39m'\u001b[39;49m,\\\n\u001b[1;32m      2\u001b[0m \u001b[39m'\u001b[39;49m\u001b[39m/Users/paigegiese/SYG/wids-2025-mupd/data/TRAIN/TRAIN_CATEGORICAL_METADATA.csv\u001b[39;49m\u001b[39m'\u001b[39;49m,\\\n\u001b[1;32m      3\u001b[0m \u001b[39m'\u001b[39;49m\u001b[39m/Users/paigegiese/SYG/wids-2025-mupd/data/TRAIN/TRAIN_QUANTITATIVE_METADATA.csv\u001b[39;49m\u001b[39m'\u001b[39;49m,\\\n\u001b[0;32m----> 4\u001b[0m \u001b[39m'\u001b[39;49m\u001b[39m/Users/paigegiese/SYG/wids-2025-mupd/data/TRAIN/TRAINING_SOLUTIONS.csv\u001b[39;49m\u001b[39m'\u001b[39;49m)\u001b[39m.\u001b[39;49mcat_df\u001b[39m#.combine_datasets(id_column='participant_id')\u001b[39;00m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'NoneType' object has no attribute 'cat_df'"
     ]
    }
   ],
   "source": [
    "ConnectomeProfiler().load_data('/Users/paigegiese/SYG/wids-2025-mupd/data/TRAIN/TRAIN_FUNCTIONAL_CONNECTOME_MATRICES.csv',\\\n",
    "'/Users/paigegiese/SYG/wids-2025-mupd/data/TRAIN/TRAIN_CATEGORICAL_METADATA.csv',\\\n",
    "'/Users/paigegiese/SYG/wids-2025-mupd/data/TRAIN/TRAIN_QUANTITATIVE_METADATA.csv',\\\n",
    "'/Users/paigegiese/SYG/wids-2025-mupd/data/TRAIN/TRAINING_SOLUTIONS.csv').cat_df#.combine_datasets(id_column='participant_id')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data loaded in 4.80 seconds\n",
      "Combined dataset created with 1213 samples and 19930 features\n",
      "Memory usage: 193.40 MB\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>participant_id</th>\n",
       "      <th>0throw_1thcolumn</th>\n",
       "      <th>0throw_2thcolumn</th>\n",
       "      <th>0throw_3thcolumn</th>\n",
       "      <th>0throw_4thcolumn</th>\n",
       "      <th>0throw_5thcolumn</th>\n",
       "      <th>0throw_6thcolumn</th>\n",
       "      <th>0throw_7thcolumn</th>\n",
       "      <th>0throw_8thcolumn</th>\n",
       "      <th>0throw_9thcolumn</th>\n",
       "      <th>...</th>\n",
       "      <th>SDQ_SDQ_Emotional_Problems</th>\n",
       "      <th>SDQ_SDQ_Externalizing</th>\n",
       "      <th>SDQ_SDQ_Generating_Impact</th>\n",
       "      <th>SDQ_SDQ_Hyperactivity</th>\n",
       "      <th>SDQ_SDQ_Internalizing</th>\n",
       "      <th>SDQ_SDQ_Peer_Problems</th>\n",
       "      <th>SDQ_SDQ_Prosocial</th>\n",
       "      <th>MRI_Track_Age_at_Scan</th>\n",
       "      <th>ADHD_Outcome</th>\n",
       "      <th>Sex_F</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>70z8Q2xdTXM3</td>\n",
       "      <td>0.093473</td>\n",
       "      <td>0.146902</td>\n",
       "      <td>0.067893</td>\n",
       "      <td>0.015141</td>\n",
       "      <td>0.070221</td>\n",
       "      <td>0.063997</td>\n",
       "      <td>0.055382</td>\n",
       "      <td>-0.035335</td>\n",
       "      <td>0.068583</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>5</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "      <td>9</td>\n",
       "      <td>11.889002</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>WHWymJu6zNZi</td>\n",
       "      <td>0.029580</td>\n",
       "      <td>0.179323</td>\n",
       "      <td>0.112933</td>\n",
       "      <td>0.038291</td>\n",
       "      <td>0.104899</td>\n",
       "      <td>0.064250</td>\n",
       "      <td>0.008488</td>\n",
       "      <td>0.077505</td>\n",
       "      <td>-0.004750</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>7.670088</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>4PAQp1M6EyAo</td>\n",
       "      <td>-0.051580</td>\n",
       "      <td>0.139734</td>\n",
       "      <td>0.068295</td>\n",
       "      <td>0.046991</td>\n",
       "      <td>0.111085</td>\n",
       "      <td>0.026978</td>\n",
       "      <td>0.151377</td>\n",
       "      <td>0.021198</td>\n",
       "      <td>0.083721</td>\n",
       "      <td>...</td>\n",
       "      <td>4</td>\n",
       "      <td>17</td>\n",
       "      <td>8</td>\n",
       "      <td>9</td>\n",
       "      <td>9</td>\n",
       "      <td>5</td>\n",
       "      <td>8</td>\n",
       "      <td>7.743896</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>obEacy4Of68I</td>\n",
       "      <td>0.016273</td>\n",
       "      <td>0.204702</td>\n",
       "      <td>0.115980</td>\n",
       "      <td>0.043103</td>\n",
       "      <td>0.056431</td>\n",
       "      <td>0.057615</td>\n",
       "      <td>0.055773</td>\n",
       "      <td>0.075030</td>\n",
       "      <td>0.001033</td>\n",
       "      <td>...</td>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>10</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>s7WzzDcmDOhF</td>\n",
       "      <td>0.065771</td>\n",
       "      <td>0.098714</td>\n",
       "      <td>0.097604</td>\n",
       "      <td>0.112988</td>\n",
       "      <td>0.071139</td>\n",
       "      <td>0.085607</td>\n",
       "      <td>0.019392</td>\n",
       "      <td>-0.036403</td>\n",
       "      <td>-0.020375</td>\n",
       "      <td>...</td>\n",
       "      <td>3</td>\n",
       "      <td>6</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>8</td>\n",
       "      <td>5</td>\n",
       "      <td>6</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1208</th>\n",
       "      <td>kTurROKp5YHQ</td>\n",
       "      <td>0.022932</td>\n",
       "      <td>0.024830</td>\n",
       "      <td>-0.006514</td>\n",
       "      <td>0.021070</td>\n",
       "      <td>0.093365</td>\n",
       "      <td>0.083369</td>\n",
       "      <td>0.010991</td>\n",
       "      <td>0.035838</td>\n",
       "      <td>0.023909</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>4</td>\n",
       "      <td>7</td>\n",
       "      <td>11.037417</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1209</th>\n",
       "      <td>FCRyMz9O6yCq</td>\n",
       "      <td>0.047078</td>\n",
       "      <td>0.135955</td>\n",
       "      <td>0.144366</td>\n",
       "      <td>0.054938</td>\n",
       "      <td>0.065760</td>\n",
       "      <td>0.113550</td>\n",
       "      <td>0.032621</td>\n",
       "      <td>0.025349</td>\n",
       "      <td>0.011848</td>\n",
       "      <td>...</td>\n",
       "      <td>4</td>\n",
       "      <td>11</td>\n",
       "      <td>6</td>\n",
       "      <td>9</td>\n",
       "      <td>7</td>\n",
       "      <td>3</td>\n",
       "      <td>6</td>\n",
       "      <td>11.571640</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1210</th>\n",
       "      <td>vm2P1hmAY1hl</td>\n",
       "      <td>0.094541</td>\n",
       "      <td>0.145280</td>\n",
       "      <td>0.061964</td>\n",
       "      <td>0.030562</td>\n",
       "      <td>0.078278</td>\n",
       "      <td>0.105857</td>\n",
       "      <td>0.043984</td>\n",
       "      <td>0.039145</td>\n",
       "      <td>0.032024</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>8</td>\n",
       "      <td>15.869952</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1211</th>\n",
       "      <td>OFxvN8lnw21w</td>\n",
       "      <td>-0.001533</td>\n",
       "      <td>0.128562</td>\n",
       "      <td>0.045000</td>\n",
       "      <td>-0.008742</td>\n",
       "      <td>0.003673</td>\n",
       "      <td>0.086494</td>\n",
       "      <td>0.090101</td>\n",
       "      <td>-0.037449</td>\n",
       "      <td>-0.026334</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>6</td>\n",
       "      <td>11.933493</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1212</th>\n",
       "      <td>9gpepMI9sj5q</td>\n",
       "      <td>-0.115224</td>\n",
       "      <td>0.147713</td>\n",
       "      <td>0.083183</td>\n",
       "      <td>0.054649</td>\n",
       "      <td>0.034871</td>\n",
       "      <td>-0.046438</td>\n",
       "      <td>-0.006836</td>\n",
       "      <td>0.058453</td>\n",
       "      <td>0.044953</td>\n",
       "      <td>...</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>8</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1213 rows × 19930 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     participant_id  0throw_1thcolumn  0throw_2thcolumn  0throw_3thcolumn  \\\n",
       "0      70z8Q2xdTXM3          0.093473          0.146902          0.067893   \n",
       "1      WHWymJu6zNZi          0.029580          0.179323          0.112933   \n",
       "2      4PAQp1M6EyAo         -0.051580          0.139734          0.068295   \n",
       "3      obEacy4Of68I          0.016273          0.204702          0.115980   \n",
       "4      s7WzzDcmDOhF          0.065771          0.098714          0.097604   \n",
       "...             ...               ...               ...               ...   \n",
       "1208   kTurROKp5YHQ          0.022932          0.024830         -0.006514   \n",
       "1209   FCRyMz9O6yCq          0.047078          0.135955          0.144366   \n",
       "1210   vm2P1hmAY1hl          0.094541          0.145280          0.061964   \n",
       "1211   OFxvN8lnw21w         -0.001533          0.128562          0.045000   \n",
       "1212   9gpepMI9sj5q         -0.115224          0.147713          0.083183   \n",
       "\n",
       "      0throw_4thcolumn  0throw_5thcolumn  0throw_6thcolumn  0throw_7thcolumn  \\\n",
       "0             0.015141          0.070221          0.063997          0.055382   \n",
       "1             0.038291          0.104899          0.064250          0.008488   \n",
       "2             0.046991          0.111085          0.026978          0.151377   \n",
       "3             0.043103          0.056431          0.057615          0.055773   \n",
       "4             0.112988          0.071139          0.085607          0.019392   \n",
       "...                ...               ...               ...               ...   \n",
       "1208          0.021070          0.093365          0.083369          0.010991   \n",
       "1209          0.054938          0.065760          0.113550          0.032621   \n",
       "1210          0.030562          0.078278          0.105857          0.043984   \n",
       "1211         -0.008742          0.003673          0.086494          0.090101   \n",
       "1212          0.054649          0.034871         -0.046438         -0.006836   \n",
       "\n",
       "      0throw_8thcolumn  0throw_9thcolumn  ...  SDQ_SDQ_Emotional_Problems  \\\n",
       "0            -0.035335          0.068583  ...                           1   \n",
       "1             0.077505         -0.004750  ...                           0   \n",
       "2             0.021198          0.083721  ...                           4   \n",
       "3             0.075030          0.001033  ...                           2   \n",
       "4            -0.036403         -0.020375  ...                           3   \n",
       "...                ...               ...  ...                         ...   \n",
       "1208          0.035838          0.023909  ...                           1   \n",
       "1209          0.025349          0.011848  ...                           4   \n",
       "1210          0.039145          0.032024  ...                           0   \n",
       "1211         -0.037449         -0.026334  ...                           0   \n",
       "1212          0.058453          0.044953  ...                           3   \n",
       "\n",
       "      SDQ_SDQ_Externalizing  SDQ_SDQ_Generating_Impact  SDQ_SDQ_Hyperactivity  \\\n",
       "0                         4                          5                      4   \n",
       "1                         0                          0                      0   \n",
       "2                        17                          8                      9   \n",
       "3                         4                          1                      4   \n",
       "4                         6                          3                      3   \n",
       "...                     ...                        ...                    ...   \n",
       "1208                      2                          4                      1   \n",
       "1209                     11                          6                      9   \n",
       "1210                      0                          0                      0   \n",
       "1211                      6                          0                      5   \n",
       "1212                      2                          0                      2   \n",
       "\n",
       "      SDQ_SDQ_Internalizing  SDQ_SDQ_Peer_Problems  SDQ_SDQ_Prosocial  \\\n",
       "0                         4                      3                  9   \n",
       "1                         0                      0                  0   \n",
       "2                         9                      5                  8   \n",
       "3                         3                      1                 10   \n",
       "4                         8                      5                  6   \n",
       "...                     ...                    ...                ...   \n",
       "1208                      5                      4                  7   \n",
       "1209                      7                      3                  6   \n",
       "1210                      2                      2                  8   \n",
       "1211                      0                      0                  6   \n",
       "1212                      3                      0                  8   \n",
       "\n",
       "      MRI_Track_Age_at_Scan  ADHD_Outcome  Sex_F  \n",
       "0                 11.889002             1      0  \n",
       "1                  7.670088             1      1  \n",
       "2                  7.743896             1      1  \n",
       "3                       NaN             1      1  \n",
       "4                       NaN             1      1  \n",
       "...                     ...           ...    ...  \n",
       "1208              11.037417             0      1  \n",
       "1209              11.571640             1      0  \n",
       "1210              15.869952             0      1  \n",
       "1211              11.933493             1      1  \n",
       "1212                    NaN             0      1  \n",
       "\n",
       "[1213 rows x 19930 columns]"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "profiler = ConnectomeProfiler()\n",
    "profiler.load_data('/Users/paigegiese/SYG/wids-2025-mupd/data/TRAIN/TRAIN_FUNCTIONAL_CONNECTOME_MATRICES.csv','/Users/paigegiese/SYG/wids-2025-mupd/data/TRAIN/TRAIN_CATEGORICAL_METADATA.csv','/Users/paigegiese/SYG/wids-2025-mupd/data/TRAIN/TRAIN_QUANTITATIVE_METADATA.csv','/Users/paigegiese/SYG/wids-2025-mupd/data/TRAIN/TRAINING_SOLUTIONS.csv')\n",
    "profiler.combine_datasets(id_column='participant_id')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "caleb",
   "language": "python",
   "name": "pipenv-python3.12"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
